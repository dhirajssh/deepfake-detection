{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization, LayerNormalization\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./train\"\n",
    "test_path = \"./test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img_dims, batch_size):\n",
    "  train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "  train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_dims, img_dims),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  test_gen = test_datagen.flow_from_directory(\n",
    "      directory=test_path,\n",
    "      target_size=(img_dims, img_dims),\n",
    "      batch_size=batch_size,\n",
    "      class_mode='binary',\n",
    "      shuffle=True\n",
    "  )\n",
    "\n",
    "  return (train_gen, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12134 images belonging to 2 classes.\n",
      "Found 2615 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_dims = 160\n",
    "epochs=10\n",
    "batch_size=16\n",
    "\n",
    "train_gen, test_gen = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (160, 160, 3)\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "# print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "image_size = 160  # We'll resize input images to this size\n",
    "patch_size = 40  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "759/759 [==============================] - 65s 78ms/step - loss: 0.6953 - accuracy: 0.6775 - top-5-accuracy: 0.9986 - val_loss: 0.4994 - val_accuracy: 0.7614 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.4802 - accuracy: 0.7796 - top-5-accuracy: 1.0000 - val_loss: 0.4486 - val_accuracy: 0.7916 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.4346 - accuracy: 0.8042 - top-5-accuracy: 1.0000 - val_loss: 0.4351 - val_accuracy: 0.8042 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "759/759 [==============================] - 64s 85ms/step - loss: 0.4145 - accuracy: 0.8128 - top-5-accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.8027 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.4166 - accuracy: 0.8114 - top-5-accuracy: 0.9999 - val_loss: 0.4118 - val_accuracy: 0.8138 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.3950 - accuracy: 0.8260 - top-5-accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.8027 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.4053 - accuracy: 0.8123 - top-5-accuracy: 1.0000 - val_loss: 0.4310 - val_accuracy: 0.7958 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "759/759 [==============================] - 64s 85ms/step - loss: 0.3736 - accuracy: 0.8329 - top-5-accuracy: 1.0000 - val_loss: 0.4000 - val_accuracy: 0.8164 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "759/759 [==============================] - 65s 86ms/step - loss: 0.3879 - accuracy: 0.8232 - top-5-accuracy: 0.9999 - val_loss: 0.3954 - val_accuracy: 0.8252 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "759/759 [==============================] - 64s 85ms/step - loss: 0.3579 - accuracy: 0.8414 - top-5-accuracy: 1.0000 - val_loss: 0.3864 - val_accuracy: 0.8161 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "759/759 [==============================] - 66s 88ms/step - loss: 0.4013 - accuracy: 0.8199 - top-5-accuracy: 0.9999 - val_loss: 0.4092 - val_accuracy: 0.8222 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3449 - accuracy: 0.8445 - top-5-accuracy: 0.9999 - val_loss: 0.3670 - val_accuracy: 0.8317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3437 - accuracy: 0.8446 - top-5-accuracy: 0.9999 - val_loss: 0.4130 - val_accuracy: 0.8214 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3563 - accuracy: 0.8393 - top-5-accuracy: 0.9997 - val_loss: 0.3813 - val_accuracy: 0.8317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.4030 - accuracy: 0.8245 - top-5-accuracy: 0.9999 - val_loss: 0.4496 - val_accuracy: 0.7874 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "759/759 [==============================] - 66s 88ms/step - loss: 0.3716 - accuracy: 0.8311 - top-5-accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.8294 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.3416 - accuracy: 0.8464 - top-5-accuracy: 1.0000 - val_loss: 0.3797 - val_accuracy: 0.8272 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3235 - accuracy: 0.8548 - top-5-accuracy: 0.9999 - val_loss: 0.3655 - val_accuracy: 0.8245 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.3474 - accuracy: 0.8437 - top-5-accuracy: 1.0000 - val_loss: 0.3625 - val_accuracy: 0.8359 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3445 - accuracy: 0.8463 - top-5-accuracy: 0.9999 - val_loss: 0.3790 - val_accuracy: 0.8145 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.3279 - accuracy: 0.8477 - top-5-accuracy: 0.9999 - val_loss: 0.3417 - val_accuracy: 0.8451 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.3123 - accuracy: 0.8593 - top-5-accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.8516 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "759/759 [==============================] - 65s 86ms/step - loss: 0.3157 - accuracy: 0.8601 - top-5-accuracy: 0.9998 - val_loss: 0.3382 - val_accuracy: 0.8486 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "759/759 [==============================] - 66s 86ms/step - loss: 0.3137 - accuracy: 0.8582 - top-5-accuracy: 0.9998 - val_loss: 0.3416 - val_accuracy: 0.8501 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "759/759 [==============================] - 65s 86ms/step - loss: 0.2992 - accuracy: 0.8673 - top-5-accuracy: 0.9998 - val_loss: 0.3424 - val_accuracy: 0.8421 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.4884 - accuracy: 0.7819 - top-5-accuracy: 0.9999 - val_loss: 0.3642 - val_accuracy: 0.8249 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3263 - accuracy: 0.8528 - top-5-accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.8126 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "759/759 [==============================] - 65s 86ms/step - loss: 0.3141 - accuracy: 0.8603 - top-5-accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.8447 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "759/759 [==============================] - 66s 86ms/step - loss: 0.2977 - accuracy: 0.8638 - top-5-accuracy: 0.9999 - val_loss: 0.3234 - val_accuracy: 0.8551 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "759/759 [==============================] - 66s 86ms/step - loss: 0.3026 - accuracy: 0.8640 - top-5-accuracy: 0.9999 - val_loss: 0.3207 - val_accuracy: 0.8455 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3334 - accuracy: 0.8491 - top-5-accuracy: 0.9998 - val_loss: 0.3304 - val_accuracy: 0.8543 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.2978 - accuracy: 0.8674 - top-5-accuracy: 1.0000 - val_loss: 0.3145 - val_accuracy: 0.8547 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2897 - accuracy: 0.8711 - top-5-accuracy: 0.9999 - val_loss: 0.3334 - val_accuracy: 0.8317 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.4005 - accuracy: 0.8037 - top-5-accuracy: 0.9999 - val_loss: 0.4645 - val_accuracy: 0.7881 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.3411 - accuracy: 0.8456 - top-5-accuracy: 0.9998 - val_loss: 0.3472 - val_accuracy: 0.8283 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "759/759 [==============================] - 68s 90ms/step - loss: 0.3017 - accuracy: 0.8662 - top-5-accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.8447 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "759/759 [==============================] - 68s 89ms/step - loss: 0.3619 - accuracy: 0.8295 - top-5-accuracy: 0.9998 - val_loss: 0.5949 - val_accuracy: 0.7618 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "759/759 [==============================] - 68s 89ms/step - loss: 0.4232 - accuracy: 0.7972 - top-5-accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.8459 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "759/759 [==============================] - 68s 90ms/step - loss: 0.3079 - accuracy: 0.8620 - top-5-accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.8302 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "759/759 [==============================] - 68s 89ms/step - loss: 0.2969 - accuracy: 0.8680 - top-5-accuracy: 0.9999 - val_loss: 0.3015 - val_accuracy: 0.8585 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "759/759 [==============================] - 68s 89ms/step - loss: 0.3051 - accuracy: 0.8620 - top-5-accuracy: 0.9998 - val_loss: 0.3155 - val_accuracy: 0.8417 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "759/759 [==============================] - 68s 89ms/step - loss: 0.2895 - accuracy: 0.8697 - top-5-accuracy: 1.0000 - val_loss: 0.3030 - val_accuracy: 0.8654 - val_top-5-accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.4123 - accuracy: 0.8052 - top-5-accuracy: 0.9998 - val_loss: 0.3777 - val_accuracy: 0.8141 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "759/759 [==============================] - 67s 88ms/step - loss: 0.3047 - accuracy: 0.8611 - top-5-accuracy: 0.9999 - val_loss: 0.3075 - val_accuracy: 0.8654 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.2856 - accuracy: 0.8727 - top-5-accuracy: 0.9998 - val_loss: 0.2848 - val_accuracy: 0.8719 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2947 - accuracy: 0.8634 - top-5-accuracy: 0.9999 - val_loss: 0.3266 - val_accuracy: 0.8528 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2744 - accuracy: 0.8780 - top-5-accuracy: 1.0000 - val_loss: 0.2730 - val_accuracy: 0.8704 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2850 - accuracy: 0.8688 - top-5-accuracy: 0.9998 - val_loss: 0.2989 - val_accuracy: 0.8650 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "759/759 [==============================] - 63s 82ms/step - loss: 0.2970 - accuracy: 0.8672 - top-5-accuracy: 0.9999 - val_loss: 0.3073 - val_accuracy: 0.8524 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2727 - accuracy: 0.8770 - top-5-accuracy: 1.0000 - val_loss: 0.2920 - val_accuracy: 0.8730 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.2913 - accuracy: 0.8687 - top-5-accuracy: 0.9999 - val_loss: 0.2848 - val_accuracy: 0.8681 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.2651 - accuracy: 0.8840 - top-5-accuracy: 0.9999 - val_loss: 0.2778 - val_accuracy: 0.8719 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "759/759 [==============================] - 63s 82ms/step - loss: 0.3144 - accuracy: 0.8557 - top-5-accuracy: 0.9999 - val_loss: 0.3186 - val_accuracy: 0.8467 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "759/759 [==============================] - 63s 82ms/step - loss: 0.2641 - accuracy: 0.8814 - top-5-accuracy: 1.0000 - val_loss: 0.2579 - val_accuracy: 0.8746 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2655 - accuracy: 0.8834 - top-5-accuracy: 0.9997 - val_loss: 0.2873 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2541 - accuracy: 0.8857 - top-5-accuracy: 1.0000 - val_loss: 0.2963 - val_accuracy: 0.8497 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2893 - accuracy: 0.8656 - top-5-accuracy: 0.9999 - val_loss: 0.3151 - val_accuracy: 0.8528 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2611 - accuracy: 0.8842 - top-5-accuracy: 0.9999 - val_loss: 0.3011 - val_accuracy: 0.8658 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2529 - accuracy: 0.8858 - top-5-accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2952 - accuracy: 0.8688 - top-5-accuracy: 0.9999 - val_loss: 0.2618 - val_accuracy: 0.8803 - val_top-5-accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "759/759 [==============================] - 66s 87ms/step - loss: 0.3265 - accuracy: 0.8470 - top-5-accuracy: 0.9998 - val_loss: 0.5499 - val_accuracy: 0.7400 - val_top-5-accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "759/759 [==============================] - 61s 80ms/step - loss: 0.3833 - accuracy: 0.8225 - top-5-accuracy: 1.0000 - val_loss: 0.2907 - val_accuracy: 0.8719 - val_top-5-accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2776 - accuracy: 0.8723 - top-5-accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.8750 - val_top-5-accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2654 - accuracy: 0.8797 - top-5-accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.8769 - val_top-5-accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2558 - accuracy: 0.8878 - top-5-accuracy: 0.9999 - val_loss: 0.3043 - val_accuracy: 0.8665 - val_top-5-accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2509 - accuracy: 0.8896 - top-5-accuracy: 1.0000 - val_loss: 0.2864 - val_accuracy: 0.8627 - val_top-5-accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "759/759 [==============================] - 63s 82ms/step - loss: 0.3265 - accuracy: 0.8425 - top-5-accuracy: 0.9999 - val_loss: 0.4271 - val_accuracy: 0.8015 - val_top-5-accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2707 - accuracy: 0.8789 - top-5-accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.8803 - val_top-5-accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2401 - accuracy: 0.8948 - top-5-accuracy: 1.0000 - val_loss: 0.2450 - val_accuracy: 0.8937 - val_top-5-accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "759/759 [==============================] - 61s 80ms/step - loss: 0.2430 - accuracy: 0.8909 - top-5-accuracy: 1.0000 - val_loss: 0.2479 - val_accuracy: 0.8822 - val_top-5-accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2411 - accuracy: 0.8917 - top-5-accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.8623 - val_top-5-accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.3068 - accuracy: 0.8648 - top-5-accuracy: 0.9999 - val_loss: 0.2815 - val_accuracy: 0.8662 - val_top-5-accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.2435 - accuracy: 0.8937 - top-5-accuracy: 0.9999 - val_loss: 0.2807 - val_accuracy: 0.8677 - val_top-5-accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.4202 - accuracy: 0.7997 - top-5-accuracy: 0.9999 - val_loss: 0.4421 - val_accuracy: 0.7866 - val_top-5-accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.3050 - accuracy: 0.8621 - top-5-accuracy: 0.9998 - val_loss: 0.3669 - val_accuracy: 0.8252 - val_top-5-accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.2627 - accuracy: 0.8815 - top-5-accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.8925 - val_top-5-accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2679 - accuracy: 0.8824 - top-5-accuracy: 0.9999 - val_loss: 0.2730 - val_accuracy: 0.8711 - val_top-5-accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.3350 - accuracy: 0.8451 - top-5-accuracy: 0.9999 - val_loss: 0.2955 - val_accuracy: 0.8608 - val_top-5-accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2505 - accuracy: 0.8878 - top-5-accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.8784 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.2370 - accuracy: 0.8945 - top-5-accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.8868 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.3457 - accuracy: 0.8420 - top-5-accuracy: 0.9998 - val_loss: 0.2771 - val_accuracy: 0.8761 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "759/759 [==============================] - 63s 84ms/step - loss: 0.2488 - accuracy: 0.8906 - top-5-accuracy: 0.9999 - val_loss: 0.2929 - val_accuracy: 0.8658 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.3763 - accuracy: 0.8260 - top-5-accuracy: 0.9999 - val_loss: 0.4749 - val_accuracy: 0.7751 - val_top-5-accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.3954 - accuracy: 0.8182 - top-5-accuracy: 0.9999 - val_loss: 0.3162 - val_accuracy: 0.8451 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2891 - accuracy: 0.8676 - top-5-accuracy: 0.9999 - val_loss: 0.3105 - val_accuracy: 0.8612 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2699 - accuracy: 0.8793 - top-5-accuracy: 0.9999 - val_loss: 0.3063 - val_accuracy: 0.8589 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "759/759 [==============================] - 63s 83ms/step - loss: 0.2499 - accuracy: 0.8903 - top-5-accuracy: 0.9999 - val_loss: 0.3258 - val_accuracy: 0.8631 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2555 - accuracy: 0.8877 - top-5-accuracy: 0.9998 - val_loss: 0.3070 - val_accuracy: 0.8616 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2442 - accuracy: 0.8934 - top-5-accuracy: 1.0000 - val_loss: 0.2338 - val_accuracy: 0.8906 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.2451 - accuracy: 0.8934 - top-5-accuracy: 1.0000 - val_loss: 0.2337 - val_accuracy: 0.8964 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "759/759 [==============================] - 62s 82ms/step - loss: 0.3268 - accuracy: 0.8476 - top-5-accuracy: 0.9999 - val_loss: 0.2437 - val_accuracy: 0.8880 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2398 - accuracy: 0.8907 - top-5-accuracy: 0.9998 - val_loss: 0.2577 - val_accuracy: 0.8818 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "759/759 [==============================] - 62s 81ms/step - loss: 0.2612 - accuracy: 0.8847 - top-5-accuracy: 0.9998 - val_loss: 0.2414 - val_accuracy: 0.8891 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "759/759 [==============================] - 61s 81ms/step - loss: 0.3980 - accuracy: 0.8084 - top-5-accuracy: 0.9998 - val_loss: 0.2718 - val_accuracy: 0.8830 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "759/759 [==============================] - 64s 84ms/step - loss: 0.2382 - accuracy: 0.8939 - top-5-accuracy: 1.0000 - val_loss: 0.2618 - val_accuracy: 0.8807 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "759/759 [==============================] - 68s 90ms/step - loss: 0.2483 - accuracy: 0.8919 - top-5-accuracy: 0.9999 - val_loss: 0.2470 - val_accuracy: 0.8929 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "759/759 [==============================] - 69s 91ms/step - loss: 0.3176 - accuracy: 0.8532 - top-5-accuracy: 0.9998 - val_loss: 0.3054 - val_accuracy: 0.8876 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "759/759 [==============================] - 69s 91ms/step - loss: 0.2446 - accuracy: 0.8933 - top-5-accuracy: 1.0000 - val_loss: 0.2253 - val_accuracy: 0.8960 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "759/759 [==============================] - 69s 91ms/step - loss: 0.2359 - accuracy: 0.8960 - top-5-accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.8841 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "759/759 [==============================] - 69s 91ms/step - loss: 0.2312 - accuracy: 0.8971 - top-5-accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.8899 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38967/1109255937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mvit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_38967/1109255937.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_5_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {round(accuracy * 100, 2)}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=test_gen,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_gen)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
